{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CIFAR10 data\n",
    "\n",
    "# import numpy as np\n",
    "# import pickle\n",
    "# import os\n",
    "\n",
    "# def unpickle(file):\n",
    "#     with open(file, 'rb') as fo:\n",
    "#         dict = pickle.load(fo, encoding='bytes')\n",
    "#     return dict\n",
    "\n",
    "# data = []\n",
    "# labels = []\n",
    "# for i in range(1, 6):\n",
    "#     filename = os.path.join('cifar-10-python','cifar-10-batches-py', 'data_batch_' + str(i))\n",
    "#     batch = unpickle(filename)\n",
    "#     for i in range(len(batch[b'labels'])):\n",
    "#         if batch[b'labels'][i] == 0 or batch[b'labels'][i] == 1:\n",
    "#             data.append(batch[b'data'][i])\n",
    "#             labels.append(batch[b'labels'][i])\n",
    "\n",
    "# data = np.array(data)\n",
    "# labels = np.array(labels)\n",
    "# # print(data.shape)\n",
    "\n",
    "# # Get the test data\n",
    "# filename = os.path.join('cifar-10-python','cifar-10-batches-py', 'test_batch')\n",
    "# batch = unpickle(filename)\n",
    "# test_data = []\n",
    "# test_labels = []\n",
    "# for i in range(len(batch[b'labels'])):\n",
    "#     if batch[b'labels'][i] == 0 or batch[b'labels'][i] == 1:\n",
    "#         test_data.append(batch[b'data'][i])\n",
    "#         test_labels.append(batch[b'labels'][i])\n",
    "\n",
    "# test_data = np.array(test_data)\n",
    "# test_labels = np.array(test_labels)\n",
    "# # print(test_data.shape)\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "def unpickle(file):\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict\n",
    "\n",
    "data = []\n",
    "labels = []\n",
    "for i in range(1,6):\n",
    "    batch = unpickle(os.path.join('cifar-10-python','cifar-10-batches-py', 'data_batch_' + str(i)))\n",
    "    data.append(batch[b'data'])\n",
    "    labels.append(batch[b'labels'])\n",
    "data = np.concatenate(data)\n",
    "labels = np.concatenate(labels)\n",
    "\n",
    "# print(data.shape)\n",
    "\n",
    "test_batch = unpickle(os.path.join('cifar-10-python','cifar-10-batches-py', 'data_batch_' + str(i)))\n",
    "test_data = test_batch[b'data']\n",
    "test_labels = np.array(test_batch[b'labels'])\n",
    "\n",
    "# print(test_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement the Conv2D layer\n",
    "class Conv2D:\n",
    "    def __init__(self, in_channels, out_channels, kernel_size):\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "\n",
    "        # Initialize the weights\n",
    "        self.weights = np.random.randn(out_channels, in_channels, kernel_size, kernel_size)\n",
    "        self.bias = np.random.randn(out_channels, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Implement the forward pass for the Conv2D layer\n",
    "        # print(x.shape)\n",
    "        # print(self.weights.shape)\n",
    "        # print(self.bias.shape)\n",
    "        self.x = x\n",
    "        # print(self.x.shape)\n",
    "        self.output = np.zeros((self.out_channels, x.shape[0] - self.kernel_size + 1, x.shape[1] - self.kernel_size + 1))\n",
    "        print(self.output.shape)\n",
    "        for i in range(self.out_channels):\n",
    "            for j in range(self.x.shape[0] - self.kernel_size + 1):\n",
    "                for k in range(self.x.shape[1] - self.kernel_size + 1):\n",
    "                    self.output[i][j][k] = np.sum(self.weights[i] * self.x[j:j + self.kernel_size, k:k + self.kernel_size]) + self.bias[i]\n",
    "        print(self.output.shape)\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        # Implement the backward pass for the Conv2D layer\n",
    "        # grad_output is the gradient from the previous layer\n",
    "        # Return the gradient for the current layer\n",
    "        self.grad_weights = np.zeros((self.out_channels, self.in_channels, self.kernel_size, self.kernel_size))\n",
    "        self.grad_bias = np.zeros((self.out_channels, 1))\n",
    "        self.grad_input = np.zeros((self.in_channels, self.x.shape[0], self.x.shape[1]))\n",
    "        for i in range(self.out_channels):\n",
    "            for j in range(self.x.shape[0] - self.kernel_size + 1):\n",
    "                for k in range(self.x.shape[1] - self.kernel_size + 1):\n",
    "                    self.grad_weights[i] += grad_output[i][j][k] * self.x[j:j + self.kernel_size, k:k + self.kernel_size]\n",
    "                    self.grad_bias[i] += grad_output[i][j][k]\n",
    "                    self.grad_input[:, j:j + self.kernel_size, k:k + self.kernel_size] += grad_output[i][j][k] * self.weights[i]\n",
    "        return self.grad_input\n",
    "    \n",
    "\n",
    "# Implement the MaxPool2D layer\n",
    "class MaxPool2D:\n",
    "    def __init__(self, kernel_size):\n",
    "        self.kernel_size = kernel_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Implement the forward pass for the MaxPool2D layer\n",
    "        # x is the input\n",
    "        # Return the output\n",
    "        self.x = x\n",
    "        self.output = np.zeros((x.shape[0], x.shape[1] // self.kernel_size, x.shape[2] // self.kernel_size))\n",
    "        for i in range(self.x.shape[0]):\n",
    "            for j in range(self.x.shape[1] // self.kernel_size):\n",
    "                for k in range(self.x.shape[2] // self.kernel_size):\n",
    "                    self.output[i][j][k] = np.max(self.x[i][j * self.kernel_size:(j + 1) * self.kernel_size, k * self.kernel_size:(k + 1) * self.kernel_size])\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        # Implement the backward pass for the MaxPool2D layer\n",
    "        # grad_output is the gradient from the previous layer\n",
    "        # Return the gradient for the current layer\n",
    "        self.grad_input = np.zeros((self.x.shape[0], self.x.shape[1], self.x.shape[2]))\n",
    "        for i in range(self.x.shape[0]):\n",
    "            for j in range(self.x.shape[1] // self.kernel_size):\n",
    "                for k in range(self.x.shape[2] // self.kernel_size):\n",
    "                    for l in range(self.kernel_size):\n",
    "                        for m in range(self.kernel_size):\n",
    "                            if self.x[i][j * self.kernel_size + l][k * self.kernel_size + m] == np.max(self.x[i][j * self.kernel_size:(j + 1) * self.kernel_size, k * self.kernel_size:(k + 1) * self.kernel_size]):\n",
    "                                self.grad_input[i][j * self.kernel_size + l][k * self.kernel_size + m] = grad_output[i][j][k]\n",
    "        return self.grad_input\n",
    "    \n",
    "\n",
    "# Implement the Linear layer\n",
    "class Linear:\n",
    "    def __init__(self, in_features, out_features):\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "\n",
    "        # Initialize the weights\n",
    "        self.weights = np.random.randn(out_features, in_features)\n",
    "        self.bias = np.random.randn(out_features, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Implement the forward pass for the Linear layer\n",
    "        # x is the input\n",
    "        # Return the output\n",
    "        self.x = x\n",
    "        self.output = np.zeros((self.out_features, 1))\n",
    "        for i in range(self.out_features):\n",
    "            self.output[i] = np.sum(self.weights[i] * self.x) + self.bias[i]\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        # Implement the backward pass for the Linear layer\n",
    "        # grad_output is the gradient from the previous layer\n",
    "        # Return the gradient for the current layer\n",
    "        self.grad_weights = np.zeros((self.out_features, self.in_features))\n",
    "        self.grad_bias = np.zeros((self.out_features, 1))\n",
    "        self.grad_input = np.zeros((self.in_features, 1))\n",
    "        for i in range(self.out_features):\n",
    "            self.grad_weights[i] = grad_output[i] * self.x\n",
    "            self.grad_bias[i] = grad_output[i]\n",
    "            self.grad_input += grad_output[i] * self.weights[i]\n",
    "        return self.grad_input\n",
    "    \n",
    "\n",
    "# Implement the ReLU layer\n",
    "class ReLU:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Implement the forward pass for the ReLU layer\n",
    "        # x is the input\n",
    "        # Return the output\n",
    "        self.x = x\n",
    "        self.output = np.zeros(x.shape)\n",
    "        for i in range(x.shape[0]):\n",
    "            for j in range(x.shape[1]):\n",
    "                for k in range(x.shape[2]):\n",
    "                    self.output[i][j][k] = max(0, x[i][j][k])\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        # Implement the backward pass for the ReLU layer\n",
    "        # grad_output is the gradient from the previous layer\n",
    "        # Return the gradient for the current layer\n",
    "        self.grad_input = np.zeros(self.x.shape)\n",
    "        for i in range(self.x.shape[0]):\n",
    "            for j in range(self.x.shape[1]):\n",
    "                for k in range(self.x.shape[2]):\n",
    "                    if self.x[i][j][k] > 0:\n",
    "                        self.grad_input[i][j][k] = grad_output[i][j][k]\n",
    "        return self.grad_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement CONV1: Kernel size (3 × 3), In channels 3, Out channels 32.\n",
    "conv1 = Conv2D(3, 32, 3)\n",
    "# Implement POOL1: Kernel size (2 × 2)\n",
    "pool1 = MaxPool2D(2)\n",
    "# Implement CONV2: Kernel size (5 × 5), In channels 32, Out channels 64\n",
    "conv2 = Conv2D(32, 64, 5)\n",
    "# Implement POOL2: Kernel size (2 × 2)\n",
    "pool2 = MaxPool2D(2)\n",
    "# Implement CONV3: Kernel size (3 × 3), In channels 64, Out channels 64.\n",
    "conv3 = Conv2D(64, 64, 3)\n",
    "# Implement FC1: Fully connected layer (also known as Linear layer) with 64 output neurons.\n",
    "fc1 = Linear(64, 64)\n",
    "# Implement FC2: Fully connected layer with 10 output neurons.\n",
    "fc2 = Linear(64, 10)\n",
    "\n",
    "# Use the categorical cross entropy loss as the loss function.\n",
    "def loss_function(y, y_hat):\n",
    "    # Implement the loss function\n",
    "    # y is the ground truth label\n",
    "    # y_hat is the output from the forward pass\n",
    "    # Return the loss\n",
    "    loss = 0\n",
    "    for i in range(y.shape[0]):\n",
    "        loss += -np.log(y_hat[i][y[i]])\n",
    "    return loss\n",
    "\n",
    "# Implement the activation function RELU where x has shape (a, b)\n",
    "def relu(x):\n",
    "    # Implement the RELU function\n",
    "    # Return the output\n",
    "    output = np.zeros(x.shape)\n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(x.shape[1]):\n",
    "            for k in range(x.shape[2]):\n",
    "                 output[i][j][k] = max(0, x[i][j][k])\n",
    "    return output\n",
    "\n",
    "# Apply RELU activation function to the output of the CONV1, CONV2, and CONV3 layers and the FC1 layer.\n",
    "\n",
    "# Implement the forward pass\n",
    "def forward(x):\n",
    "    # Implement the forward pass\n",
    "    # x is the input\n",
    "    # Return the output\n",
    "    x = conv1.forward(x)\n",
    "    x = relu(x)\n",
    "    x = pool1.forward(x)\n",
    "    # x = np.expand_dims(x,axis=3)\n",
    "    x = conv2.forward(x)\n",
    "    x = relu(x)\n",
    "    x = pool2.forward(x)\n",
    "    x = conv3.forward(x)\n",
    "    x = relu(x)\n",
    "    x = x.reshape((x.shape[0], x.shape[3]))\n",
    "    x = fc1.forward(x)\n",
    "    x = relu(x)\n",
    "    x = fc2.forward(x)\n",
    "    return x\n",
    "\n",
    "# Implement the backward pass\n",
    "def backward(y, y_hat):\n",
    "    # Implement the backward pass\n",
    "    # y is the ground truth label\n",
    "    # y_hat is the output from the forward pass\n",
    "    # Return the gradients for the parameters\n",
    "    grad_output = np.zeros((y_hat.shape[0], y_hat.shape[1]))\n",
    "    for i in range(y.shape[0]):\n",
    "        grad_output[i][y[i]] = -1 / y_hat[i][y[i]]\n",
    "    grad_output = fc2.backward(grad_output)\n",
    "    grad_output = relu.backward(grad_output)\n",
    "    grad_output = fc1.backward(grad_output)\n",
    "    grad_output = relu.backward(grad_output)\n",
    "    grad_output = grad_output.reshape((grad_output.shape[0], 1, 1, grad_output.shape[1]))\n",
    "    grad_output = conv3.backward(grad_output)\n",
    "    grad_output = relu.backward(grad_output)\n",
    "    grad_output = pool2.backward(grad_output)\n",
    "    grad_output = conv2.backward(grad_output)\n",
    "    grad_output = relu.backward(grad_output)\n",
    "    grad_output = pool1.backward(grad_output)\n",
    "    grad_output = conv1.backward(grad_output)\n",
    "    return grad_output\n",
    "\n",
    "# Implement the Adam optimizer\n",
    "class Adam:\n",
    "    def __init__(self, parameters, learning_rate, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "        # parameters is a list of parameters to optimize\n",
    "        # learning_rate is the learning rate\n",
    "        # beta1, beta2, epsilon are hyperparameters of the Adam optimizer\n",
    "        self.parameters = parameters\n",
    "        self.learning_rate = learning_rate\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        self.m = [np.zeros(p.shape) for p in parameters]\n",
    "        self.v = [np.zeros(p.shape) for p in parameters]\n",
    "        self.t = 0\n",
    "\n",
    "    def step(self, gradients):\n",
    "        # Implement the Adam optimizer\n",
    "        # gradients is a list of gradients for the parameters\n",
    "        # Return the updated parameters\n",
    "        self.t += 1\n",
    "        for i in range(len(self.parameters)):\n",
    "            self.m[i] = self.beta1 * self.m[i] + (1 - self.beta1) * gradients[i]\n",
    "            self.v[i] = self.beta2 * self.v[i] + (1 - self.beta2) * gradients[i] * gradients[i]\n",
    "            m_hat = self.m[i] / (1 - self.beta1 ** self.t)\n",
    "            v_hat = self.v[i] / (1 - self.beta2 ** self.t)\n",
    "            self.parameters[i] -= self.learning_rate * m_hat / (np.sqrt(v_hat) + self.epsilon)\n",
    "        return self.parameters\n",
    "\n",
    "\n",
    "# Complete the training code, and train for 20 epochs using an Adam optimizer with learning rate 0.001 and batch size 32.\n",
    "\n",
    "# Using the Adam class, train the data \n",
    "\n",
    "def train(x, y, model, loss_function, optimizer, batch_size, epochs):\n",
    "    # x is the training data\n",
    "    # y is the training labels\n",
    "    # model is the model to train\n",
    "    # loss_function is the loss function\n",
    "    # optimizer is the optimizer\n",
    "    # batch_size is the batch size\n",
    "    # epochs is the number of epochs\n",
    "    # Return the trained model\n",
    "    for epoch in range(epochs):\n",
    "        print('Epoch: ', epoch + 1)\n",
    "        for i in range(0, x.shape[0], batch_size):\n",
    "            x_batch = x[i:i + batch_size]\n",
    "            y_batch = y[i:i + batch_size]\n",
    "            y_hat = forward(x_batch)\n",
    "            loss = loss_function(y_batch, y_hat)\n",
    "            print('Loss: ', loss)\n",
    "            gradients = backward(y_batch, y_hat)\n",
    "            optimizer.step(gradients)\n",
    "    return model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"4\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"4\"\n",
    "\n",
    "# model = train(data[0], labels, [conv1, conv2, conv3, fc1, fc2], loss_function, Adam([conv1.weights, conv1.bias, conv2.weights, conv2.bias, conv3.weights, conv3.bias, fc1.weights, fc1.bias, fc2.weights, fc2.bias], 0.001), batch_size=32, epochs=20)\n",
    "# Train the model for one image\n",
    "\n",
    "model = train(data, labels, [conv1, conv2, conv3, fc1, fc2], loss_function, Adam([conv1.weights, conv1.bias, conv2.weights, conv2.bias, conv3.weights, conv3.bias, fc1.weights, fc1.bias, fc2.weights, fc2.bias], 0.001), batch_size=32, epochs=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
